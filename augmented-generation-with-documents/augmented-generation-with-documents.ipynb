{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using AI21 Labs Jamba API Documents to improve your results.\n",
    "This notebook focuses on Augmented Generation technique, using AI21 Labs Jamba 1.5 Large model and the 'documents' parameter input to produce more accurate, informative, and contextually relevant outputs. The Augmented Generation is often executed as part of a Retrieval-Augmented Generation (RAG) solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Overview\n",
    "RAG is a technique in natural language processing that combines the strengths of retrieval-based and generative models. Here's a breakdown of how it works and its key components:\n",
    "\n",
    "1. **Retrieval-Based Models**: These models retrieve relevant information from a large dataset or knowledge base. They use techniques like similarity search or keyword matching to find the most relevant pieces of information.\n",
    "\n",
    "\n",
    "2. **Generative Models**: These models generate new content based on the input they receive. They are typically based on neural networks, such as transformers, and can create coherent and contextually relevant text.\n",
    "\n",
    "\n",
    "3. **Combining Both**: RAG integrates these two approaches by first retrieving relevant information and then using that information to generate a response. This allows the model to produce more accurate and informative outputs by leveraging the retrieved data.\n",
    "\n",
    "\n",
    "### How RAG Works:\n",
    "\n",
    "1. **Retrieval Phase**: The model searches for relevant documents or pieces of information from a large dataset. This could involve searching through Wikipedia articles, news archives, or any other text corpus.\n",
    "\n",
    "\n",
    "2. **Generation Phase**: The retrieved information is then fed into a generative model, which uses it to produce a coherent and contextually appropriate response. The generative model can be fine-tuned to ensure that the generated text is relevant and accurate.\n",
    "\n",
    "\n",
    "### Benefits of RAG:\n",
    "\n",
    "* **Improved Accuracy**: By retrieving relevant information, RAG can produce more accurate and contextually appropriate responses.\n",
    "* **Enhanced Context Understanding**: The model can better understand the context and nuances of the input by referring to external sources.\n",
    "* **Scalability**: RAG can leverage large datasets without needing to store all the information in the model, making it more scalable.\n",
    "\n",
    "### Applications of RAG:\n",
    "\n",
    "* **Question Answering**: Providing detailed and accurate answers to user queries by retrieving and generating relevant information.\n",
    "* **Content Creation**: Generating articles, reports, or summaries that are well-researched and informative.\n",
    "* **Chatbots and Virtual Assistants**: Enhancing the capabilities of conversational agents by providing them with access to a vast knowledge base.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Use Case Overview - Financial Document Analysis\n",
    "As part of their research, Financial Analysts analyze and ask questions about the company's performance, using multiple sources of information, including the company's financial reports.\n",
    "This notebook demonstrates how questions asked by a Financial Analyst are answered by AI21 Labs Jamba 1.5 model based on the provided documents.\n",
    "The sample data used in this notebook includes 10K filings for Alphabet Inc. between the years 2021-2023 ([example filing report](https://www.sec.gov/Archives/edgar/data/1652044/000165204422000019/goog-20211231.htm))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - Query using 'documents' parameter\n",
    "The next code section demonstrates how a document is used to augment the generation of a response. The Jamba 1.5 API includes a 'documents' parameter that allows the client to explicitly define the contents of the documents to augment the generation of  the response.\n",
    "More information on this parameter can be found in the [API docs](https://docs.ai21.com/reference/jamba-15-api-ref#:~:text=in%20the%20prompt.-,documents,-%3A%20%5Barray%20of)\n",
    "\n",
    "In a full RAG solution, the content of the document can be retrieved using semantic search, for example [AI21 Labs Semantic Search API](https://docs.ai21.com/reference/semantic-search-api-ref)\n",
    "\n",
    "In this notebook, the document content is already extracted into the sample data file: goog-20231231-complete-document.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set AI21 API Key as evironment variable\n",
    "import os\n",
    "os.environ['AI21_API_KEY'] = 'Your AI21 API Key here... (access your key at https://studio.ai21.com/account/api-key)'\n",
    "\n",
    "\n",
    "if os.environ['AI21_API_KEY'] == 'Your AI21 API Key here... (access your key at https://studio.ai21.com/account/api-key)':\n",
    "    print('Please set your AI21 API Key in the environment variable AI21_API_KEY')\n",
    "    print('You can get your key at https://studio.ai21.com/account/api-key')\n",
    "    print('For example, you can set your key using the following command:')\n",
    "    print('os.environ[\\'AI21_API_KEY\\'] = \\'your-key-here\\'')\n",
    "    print('If you do not have an AI21 account, you can sign up for free at https://studio.ai21.com/signup')\n",
    "    raise UserWarning('Exit Early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file contents\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set template for the prompt\n",
    "prompt_template = \"Question: {question}\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1 - Query using Documents parameter.\n",
    "# Question to ask the model\n",
    "question1 = \"What was Alphabet's operating margin in 2023?\"\n",
    "\n",
    "prompt_content = prompt_template.format(question=question1)\n",
    "\n",
    "document_content = read_txt('sample-docs/goog-20231231-complete-document.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jamba's answer:  Alphabet's operating margin in 2023 was 27%.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ai21 import AI21Client\n",
    "from ai21.models.chat import ChatMessage\n",
    "\n",
    "client = AI21Client(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"AI21_API_KEY\"),\n",
    ")\n",
    "completion_response = client.chat.completions.create(\n",
    "  model=\"jamba-1.5-large\",\n",
    "  messages=[ChatMessage(\n",
    "    content=prompt_content,\n",
    "    role=\"user\",\n",
    "  )],\n",
    "  num_results=1,\n",
    "  max_tokens=200,\n",
    "  temperature=0.0,\n",
    "  top_p=1,\n",
    "  stop_sequences=[],\n",
    "  documents=[\n",
    "    {\"content\":document_content}],\n",
    ")\n",
    "\n",
    "print(\"Jamba's answer: \", completion_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2 - Using multiple segments and metadata\n",
    "When using large documents that go exceed the [256k token context window](https://www.ai21.com/blog/announcing-jamba-model-family#:~:text=Long-,context,-handling%3A%20With), or to achieve higher efficiency, you may be using portions of the documents relevant to your query. In RAG solution, that function is often performed by your semantic search module, for example [AI21 Labs Semantic Search](https://docs.ai21.com/reference/semantic-search-api-ref).\n",
    "The next code section shows how you can pass each relevant segment using multiple document objects included in the 'documents' parameter list.\n",
    "\n",
    "Each document can also include ['metadata' parameters](https://docs.ai21.com/reference/jamba-15-api-ref#:~:text=of%20this%20%22document%22.-,metadata,-%3A%20%5Barray%20of) as key/value pairs that you can reference in your prompt to align the response to specific segments.\n",
    "In this example, we are providing segments related to 'operating margin' for company 'Alphabet', and identifying the document's relevant year to narrow down the response to specific segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2 - Using multiple segments and metadata\n",
    "# Question to ask the model\n",
    "question2 = \"What was Alphabet's operating margin in 2023?\"\n",
    "\n",
    "prompt_content = prompt_template.format(question=question2)\n",
    "\n",
    "document_segment1_content = read_txt('sample-docs/goog-20211231-segment.txt')\n",
    "document_segment1_metadata = [{\"key\":\"year\", \"value\": \"2021\"}, \n",
    "                                {\"key\":\"company\", \"value\": \"Alphabet\"}, \n",
    "                                {\"key\":\"document\", \"value\": \"FORM 10-K - operating margin\"},]\n",
    "\n",
    "document_segment2_content = read_txt('sample-docs/goog-20221231-segment.txt')\n",
    "document_segment2_metadata = [{\"key\":\"year\", \"value\": \"2022\"}, \n",
    "                                {\"key\":\"company\", \"value\": \"Alphabet\"}, \n",
    "                                {\"key\":\"document\", \"value\": \"FORM 10-K - operating margin\"},]\n",
    "\n",
    "document_segment3_content = read_txt('sample-docs/goog-20231231-segment.txt')\n",
    "document_segment3_metadata = [{\"key\":\"year\", \"value\": \"2023\"}, \n",
    "                                {\"key\":\"company\", \"value\": \"Alphabet\"}, \n",
    "                                {\"key\":\"document\", \"value\": \"FORM 10-K - operating margin\"},]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet's operating margin in 2022 was 26%.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ai21 import AI21Client\n",
    "from ai21.models.chat import ChatMessage\n",
    "\n",
    "client = AI21Client(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"AI21_API_KEY\"),\n",
    ")\n",
    "completion_response = client.chat.completions.create(\n",
    "  model=\"jamba-1.5-large\",\n",
    "  messages=[ChatMessage(\n",
    "    content=\"Question: What was Alphabet's operating margin in 2022?\\nAnswer:\",\n",
    "    role=\"user\",\n",
    "  )],\n",
    "  num_results=1,\n",
    "  max_tokens=200,\n",
    "  temperature=0.0,\n",
    "  top_p=1,\n",
    "  stop_sequences=[],\n",
    "  documents=[\n",
    "    {\"content\": document_segment1_content,\n",
    "     \"metadata\": document_segment1_metadata},\n",
    "    {\"content\": document_segment2_content,\n",
    "     \"metadata\": document_segment2_metadata},\n",
    "    {\"content\": document_segment3_content,\n",
    "     \"metadata\": document_segment3_metadata},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAQ\n",
    "\n",
    "Q: Should I use Jamba 1.5 mini or large?\\\n",
    "A: You should evaluate your use case in context of your requirements, and assess against the model's characteristics. The details of the two models are described [here](https://arxiv.org/html/2408.12570v1#abstract)\n",
    "\n",
    "Q: Should I remove formatting and non-textual elements (like HTML) from my documents prior to sending as input?\\\n",
    "A: You should test the impact of the formatting elements on your results. In general, in cases where you require more efficient and smaller request data size (smaller number of input tokens), you should consider removing them to optimize for efficiency. \n",
    "\n",
    "Q: Does the 'documents' parameter's content impact the total allowed number of input tokens?\\\n",
    "A: Yes, the total input token count includes the tokens from the documents parameter content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fnac-legacy-v2-p312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
