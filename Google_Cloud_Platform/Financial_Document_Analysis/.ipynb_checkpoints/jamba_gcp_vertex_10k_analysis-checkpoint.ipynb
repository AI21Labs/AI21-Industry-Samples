{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26397bea-29e7-4185-b03f-0ae80685de9b",
   "metadata": {},
   "source": [
    "# Using Jamba Instruct on GCP' Vertex AI for Analyzing Financial Documents \n",
    "Jamba family models can be used to analyze financial documents. In this notebook, we will first part of JPMorgan Chas's 10K filing (downloaded from [here](https://d18rn0p25nwr6d.cloudfront.net) )\n",
    "and can then ask questions about it. This data set has around 80 pages of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e69372-d46b-42f7-8588-082a51e77084",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-auth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2469f0ac-becf-4c94-84c8-d68c9cb15f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import subprocess\n",
    "\n",
    "# Function to read the contents of the files\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Paths to the text files\n",
    "document = '10k.txt'\n",
    "\n",
    "# Function to call GCP AI model\n",
    "from google.auth import default\n",
    "import google.auth.transport.requests\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def call_gcp_vertex_jamba(user_message,model_name, project_id, location,  max_tokens):\n",
    "    \"\"\"\n",
    "    Calls the GCP Vertex AI Jamba model and returns the response text.\n",
    "\n",
    "    Args:\n",
    "    - model_name (str): Name of the model to use (e.g., 'jamba-1.5-large').\n",
    "    - project_id (str): Google Cloud project ID.\n",
    "    - location (str): Location of the model (e.g., 'us-central1').\n",
    "    - user_message (str): The message content to send to the model.\n",
    "    - max_tokens (int): Maximum number of tokens in the response.\n",
    "\n",
    "    Returns:\n",
    "    - response_text (str): The response text from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the credentials and create an auth request\n",
    "    credentials, _ = default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    credentials.refresh(auth_req)\n",
    "    access_token = credentials.token\n",
    "    temperature=.7\n",
    "\n",
    "    # Set up endpoint and headers\n",
    "    endpoint = f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/publishers/ai21/models/{model_name}:rawPredict\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Set up request data\n",
    "    data = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": user_message}],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\":temperature\n",
    "    }\n",
    "\n",
    "    # Make the POST request\n",
    "    response = requests.post(endpoint, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    # Return the response text\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# Function to handle retries\n",
    "def call_gcp_vertex_jamba_with_retries(prompt,project_id,model_name,location, **kwargs):\n",
    "    attempts = 0\n",
    "    while attempts < 5:\n",
    "        try:\n",
    "            model_name\n",
    "            return call_gcp_vertex_jamba(prompt,project_id,location, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            attempts += 1\n",
    "            print(f\"Attempt {attempts}: Failed to call API, retrying in 3 seconds...\")\n",
    "            time.sleep(3)\n",
    "    raise Exception(\"Failed to complete the API call after 5 attempts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3bfe13-2244-486e-add5-925b837e6d83",
   "metadata": {},
   "source": [
    "## Ask a Question\n",
    "Next we will ask a question from the document, passing the entire content of the document to the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d22a19-6f7b-476e-b01c-3cdf219c30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What was Amazon's revenue generating activity in 2019?\"\n",
    "document_content = read_file(document)\n",
    "q_a_prompt = f\"\"\"\n",
    "You are an excellent research assistant. Based the following \"Document Content\" do your best to answer the question posed.\n",
    "Keep your answer strictly grounded in the document, and if the answer cannot be found in the document, just say \"I do not know\"\n",
    "\n",
    "Document Content:\n",
    "\n",
    "{document_content}\n",
    "\n",
    "Question: \n",
    "{question}\n",
    "\n",
    "Answer to Question:\n",
    "\"\"\"\n",
    "\n",
    "# Call the GCP model in vertex\n",
    "project_id=\"<YOUR PROJECT ID>\"\n",
    "model_name=\"Jamba-1.5-Mini\" #or switch to Jamba-1.5-Large\n",
    "location=\"us-central1\"\n",
    "answer = call_gcp_vertex_jamba_with_retries(q_a_prompt,model_name,project_id,location, temperature=0.7)\n",
    "\n",
    "# Print the answer\n",
    "print(\"Answer:\")\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2",
   "language": "python",
   "name": "python-3.12.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
