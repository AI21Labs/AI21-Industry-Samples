{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecddc0be-4c2b-41d6-8cd2-dd02bcf5f733",
   "metadata": {},
   "source": [
    "# Fine Tune Jamba Base on Amazon SageMaker\n",
    "\n",
    "This notebook shows how to fine tune Jamba Base on Amazon SageMaker. This was tested on a SageMaker ml.p4d.24xlarge notebook instance. Make sure the notebook instance has at least 200GB of storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901dba88-f7ff-4bfb-9572-48ec50a80778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install --root-user-action=ignore  setuptools==69.5.1\n",
    "!pip3 install --root-user-action=ignore trl==0.9.4\n",
    "!pip3 install --root-user-action=ignore peft==0.5.0\n",
    "!pip3 install --root-user-action=ignore transformers==4.42.3\n",
    "!pip3 install --root-user-action=ignore tensorboard==2.17.0\n",
    "!pip3 install --root-user-action=ignore deepspeed==0.14.4\n",
    "!pip3 install --root-user-action=ignore accelerate==0.32.1\n",
    "!pip install mamba-ssm causal-conv1d>=1.2.0\n",
    "!pip install -U bitsandbytes\n",
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b4961-353c-476e-bcb9-865b1e8af4ec",
   "metadata": {},
   "source": [
    "Due to the size of the Jamba Base model (~100 GB) we need to move the huggingface cache to the EBS volume that is supporting the SageMaker notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a539320d-b88d-44bc-a554-ec5ac31edc01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME set to: /home/ec2-user/SageMaker/.cache/huggingface\n",
      "TRANSFORMERS_CACHE set to: /home/ec2-user/SageMaker/.cache/transformers\n",
      "HF_DATASETS_CACHE set to: /home/ec2-user/SageMaker/.cache/huggingface_datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set the path for Hugging Face cache\n",
    "hf_home_path = os.path.expanduser('~/SageMaker/.cache/huggingface')\n",
    "os.environ['HF_HOME'] = hf_home_path\n",
    "print(f\"HF_HOME set to: {hf_home_path}\")\n",
    "\n",
    "# Set the path for Transformers cache\n",
    "tf_home_path = os.path.expanduser('~/SageMaker/.cache/transformers')\n",
    "os.environ['TRANSFORMERS_CACHE'] = tf_home_path\n",
    "print(f\"TRANSFORMERS_CACHE set to: {tf_home_path}\")\n",
    "\n",
    "# Optional: Set the datasets cache\n",
    "datasets_cache_path = os.path.expanduser('~/SageMaker/.cache/huggingface_datasets')\n",
    "os.environ['HF_DATASETS_CACHE'] = datasets_cache_path\n",
    "print(f\"HF_DATASETS_CACHE set to: {datasets_cache_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b97281-1d33-451e-a4cf-8268d60d1a6c",
   "metadata": {},
   "source": [
    "## Perform finetuneing\n",
    "Next we will perform the actual fine tuning using Lora. Running 3 epochs took about 12 minutes on a ml.p4d.24xlarge notebook instance. You can uncomment the quantization component if you want to quantize the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598200ee-4cfa-4e96-9614-ef1e075c07d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments,  BitsAndBytesConfig\n",
    "import os\n",
    "from mamba_ssm.ops.selective_scan_interface import mamba_inner_fn, selective_scan_fn\n",
    "from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\", device_map='auto')\n",
    "\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "#quantization_config = BitsAndBytesConfig(load_in_8bit=True,\n",
    "#                                         llm_int8_skip_modules=[\"mamba\"])\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             attn_implementation=\"flash_attention_2\",\n",
    "                                             #quantization_config=quantization_config,\n",
    "                                             device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")\n",
    "\n",
    "dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-3\n",
    ")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"embed_tokens\", \"x_proj\", \"in_proj\", \"out_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    bias=\"none\"\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"quote\",\n",
    ")\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
